{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "from math import log\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim import corpora, models, similarities\n",
    "import bs4\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "импортируем необходимое импортируемое, отдельно стоп-слова из nltk.download(), включим логирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    text = bs4.BeautifulSoup(bs4.UnicodeDammit(text).unicode_markup, 'lxml').get_text()\n",
    "    return [wnl.lemmatize(t) for t in text.lower().replace(',','').split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "создаем функцию для лемматизации + токенизации + дехтмлизации текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reut2-015.sgm\n",
      "reut2-003.sgm\n",
      "reut2-020.sgm\n",
      "reut2-001.sgm\n",
      "reut2-009.sgm\n",
      "reut2-011.sgm\n",
      "reut2-000.sgm\n",
      "reut2-007.sgm\n",
      "reut2-006.sgm\n",
      "reut2-016.sgm\n",
      "reut2-010.sgm\n",
      "reut2-018.sgm\n",
      "reut2-019.sgm\n",
      "reut2-012.sgm\n",
      "reut2-002.sgm\n",
      "reut2-004.sgm\n",
      "reut2-008.sgm\n",
      "reut2-005.sgm\n",
      "reut2-013.sgm\n",
      "reut2-021.sgm\n",
      "reut2-017.sgm\n",
      "reut2-014.sgm\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for filename in os.listdir(\"/home/tigran/Roba/pyworks/modomawka/no4/\"):\n",
    "    with  open(\"/home/tigran/Roba/pyworks/modomawka/no4/\" + filename, 'r', encoding = 'ISO-8859-1') as f: \n",
    "        print(filename)\n",
    "        texts.append(preprocess(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "загрузим коллекцию обработанных текстов (названия файлов выводятся для наглядности (и для поиска багов, был битый текст))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Dictionary(148212 unique tokens: ['09:51:38.05', 'f5883reute', 'energy-based', 'mcghee', 'bc-warner-communications']...)\n",
      "Filtered: Dictionary(11122 unique tokens: ['vulnerability', 'supply-demand', 'olivetti', '302', '13.50']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "print('Original: {}'.format(dictionary))\n",
    "dictionary.filter_extremes(no_below = 5, no_above = 0.5, keep_n=None)\n",
    "dictionary.save('nips.dict')\n",
    "print('Filtered: {}'.format(dictionary))\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('nips.mm', corpus) # store on disc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "создадим словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(model, corpus):\n",
    "    corpus_length = 0\n",
    "    log_likelihood = 0\n",
    "    topic_profiles = model.state.get_lambda() / np.sum(model.state.get_lambda(), axis=1)[:, np.newaxis]\n",
    "    for document in corpus:\n",
    "        gamma, _ = model.inference([document])\n",
    "        document_profile = gamma / np.sum(gamma)\n",
    "        for term_id, term_count in document:\n",
    "            corpus_length += term_count\n",
    "            term_probability = np.dot(document_profile, topic_profiles[:, term_id])\n",
    "            log_likelihood += term_count * log(term_probability)\n",
    "    perplexity = np.exp(-log_likelihood / corpus_length)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "определим функцию перплексии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 7074.609683705144\n",
      "Evaluation time: 0.25847225189208983\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "model = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=30, chunksize=50, update_every=1, passes=2)\n",
    "print('Perplexity: {}'.format(perplexity(model, corpus)))\n",
    "print('Evaluation time: {}'.format((time()-start) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "посчитали модель на 30 топиках. хоть перплексити в абсолютном выражении и просто попугаи, но сдается мне, что их чересчур много. дальше будет показано, что модель и вправду очень плохая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.004*--- + 0.004*apr + 0.002*cq + 0.001*gq + 0.001*twa + 0.001*0 + 0.001*mth + 0.001*comex + 0.001*louvre + 0.001*platinum'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topic(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "дааа, судя по коэффициентам, что-то у модели не задалось..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ---     \n",
      "    ---     \n",
      "    apr     \n",
      "    ---     \n",
      "   magma    \n",
      "    ---     \n",
      " jefferies  \n",
      "    apr     \n",
      "    ---     \n",
      "    ---     \n",
      "\n",
      "    apr     \n",
      "    adb     \n",
      "    ---     \n",
      "    apr     \n",
      " fairchild  \n",
      "    apr     \n",
      "    apr     \n",
      "    ---     \n",
      "    apr     \n",
      "    apr     \n",
      "\n",
      "    g-7     \n",
      "    csr     \n",
      "     cq     \n",
      "    csr     \n",
      "interconnect\n",
      "     cq     \n",
      "    g-7     \n",
      "    csr     \n",
      "     cq     \n",
      "   louvre   \n",
      "\n",
      "   louvre   \n",
      "    taft    \n",
      "    twa     \n",
      "borg-warner \n",
      "    frn     \n",
      "   louvre   \n",
      "   louvre   \n",
      "     0      \n",
      "     gq     \n",
      "   caesar   \n",
      "\n",
      "    twa     \n",
      "     cq     \n",
      "   dixons   \n",
      " jefferies  \n",
      "   parker   \n",
      "     gq     \n",
      "    twa     \n",
      "     gq     \n",
      "  gillette  \n",
      "    csr     \n",
      "\n",
      "     0      \n",
      "    apr     \n",
      "   caesar   \n",
      "   dixons   \n",
      "  rudolph   \n",
      "     0      \n",
      "    0/92    \n",
      "     cq     \n",
      "  noranda   \n",
      "    twa     \n",
      "\n",
      "   eddie    \n",
      "     ai     \n",
      "     gq     \n",
      "     0      \n",
      "    taft    \n",
      "    csr     \n",
      "    ---     \n",
      "    adb     \n",
      "     0      \n",
      "    g-7     \n",
      "\n",
      "     cq     \n",
      "   dixons   \n",
      "    taft    \n",
      "     cq     \n",
      "    ---     \n",
      "    unc     \n",
      "    taft    \n",
      "   comex    \n",
      "     ai     \n",
      "     cq     \n",
      "\n",
      "     gq     \n",
      "    twa     \n",
      " jefferies  \n",
      "    gaf     \n",
      " greenwell  \n",
      "    g-7     \n",
      "  noranda   \n",
      "    g-7     \n",
      "   comex    \n",
      "    5.00    \n",
      "\n",
      "    adb     \n",
      "   caesar   \n",
      "    0/92    \n",
      "    twa     \n",
      "    pin     \n",
      "    adb     \n",
      "    adb     \n",
      "    twa     \n",
      "    g-7     \n",
      "   dixons   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for position in range(10):\n",
    "    for topic in range(10):\n",
    "        print(model.show_topic(topic)[position][1].center(12, ' ')) \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "честное слово, не разобрался, как вывести слова красиво в шеренгу, но это и не поможет - модель получилась, мягко скажем, отвратная. сказывается и плохой препроцессинг (был сделан вслепую, кроме обработки хтмл), и куцый набор текстов (я находился в крайне сложном положении из-за необходимости загрузиться с арх линукса на внешнем диске, на котором не работали почему-то pacman и невозможно было поставить нормальный архиватор, работающий с .rar и .zip, поэтому выбор на репозитории UCI был крайне ограничен. плюс предлагаемые данные bag of words мне не удалось заставить работать с этим алгоритмом из-за отсутствия id2word ключей в словаре (там уже готовые словари и числовые пары слово-частота), и я не уверен, что это вообще возможно. \n",
    "Интерпретируя полученные результаты, можно сказать, что они интерпретации поддаются только в негативном ключе - смысла в них нет. Попробуем интереса ради переобучиться на более сложной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 4.752013158798218\n",
      "Perplexity: 6156.594719094331\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "model2 = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=100, update_every=0, passes=40)\n",
    "print('Evaluation time: {}'.format((time()-start) / 60))\n",
    "print('Perplexity: {}'.format(perplexity(model2, corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, модель считалась ощутимо дольше, и перплексия стала на тысячу попугаев меньше - однако, их все равно многовато. Оценим, глянув на топики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"0.010*borg-warner + 0.007*texaco's + 0.006*gaf + 0.006*dome's + 0.006*uaw + 0.004*adb + 0.004*fmc + 0.003*crazy + 0.003*eddie + 0.003*reebok\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.print_topic(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "увы, сколько-нибудь более осмысленной модель не стала. слишком малы данные и плох препроцессинг для получения осмысленных результатов"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
